{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFBWzm4nIYYxKDObbB/JQB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Converting PDF to DOCX using GEMINI"
      ],
      "metadata": {
        "id": "RPZEJUUO3sRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TIcZoteve4ww",
        "outputId": "6f8be785-1b7f-47f5-ce4a-b19ad4c82934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.3.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, pdf2image\n",
            "Successfully installed pdf2image-1.17.0 python-docx-1.2.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,853 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,768 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,142 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Fetched 23.6 MB in 2s (11.6 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 1s (332 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image python-docx\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "P5Yo9MHDJkYY",
        "outputId": "42de600f-f29a-4e07-e19e-c77c4af4982c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages in PDF: 49\n",
            "Extracting page 1...\n",
            "Uploaded page 1 as: https://generativelanguage.googleapis.com/v1beta/files/m48cx1ncc9me on attempt 1\n",
            "Deleted temporary file: files/m48cx1ncc9me\n",
            "Extracting page 2...\n",
            "Uploaded page 2 as: https://generativelanguage.googleapis.com/v1beta/files/4bh0cvr8ehk2 on attempt 1\n",
            "Deleted temporary file: files/4bh0cvr8ehk2\n",
            "Extracting page 3...\n",
            "Uploaded page 3 as: https://generativelanguage.googleapis.com/v1beta/files/0ys3w4trip9c on attempt 1\n",
            "Deleted temporary file: files/0ys3w4trip9c\n",
            "Extracting page 4...\n",
            "Uploaded page 4 as: https://generativelanguage.googleapis.com/v1beta/files/w5f4t7vbvyrv on attempt 1\n",
            "Deleted temporary file: files/w5f4t7vbvyrv\n",
            "Extracting page 5...\n",
            "Uploaded page 5 as: https://generativelanguage.googleapis.com/v1beta/files/e9pxrhaa3nlc on attempt 1\n",
            "Deleted temporary file: files/e9pxrhaa3nlc\n",
            "Extracting page 6...\n",
            "Uploaded page 6 as: https://generativelanguage.googleapis.com/v1beta/files/sp92hpdupi5i on attempt 1\n",
            "Deleted temporary file: files/sp92hpdupi5i\n",
            "Extracting page 7...\n",
            "Uploaded page 7 as: https://generativelanguage.googleapis.com/v1beta/files/iagv23c974ek on attempt 1\n",
            "Deleted temporary file: files/iagv23c974ek\n",
            "Extracting page 8...\n",
            "Uploaded page 8 as: https://generativelanguage.googleapis.com/v1beta/files/ze2x22sueopp on attempt 1\n",
            "Deleted temporary file: files/ze2x22sueopp\n",
            "Extracting page 9...\n",
            "Uploaded page 9 as: https://generativelanguage.googleapis.com/v1beta/files/p0jriuta6psm on attempt 1\n",
            "Deleted temporary file: files/p0jriuta6psm\n",
            "Extracting page 10...\n",
            "Uploaded page 10 as: https://generativelanguage.googleapis.com/v1beta/files/g9evu7dwhj5y on attempt 1\n",
            "Deleted temporary file: files/g9evu7dwhj5y\n",
            "Extracting page 11...\n",
            "Uploaded page 11 as: https://generativelanguage.googleapis.com/v1beta/files/2dbxl9wjladh on attempt 1\n",
            "Deleted temporary file: files/2dbxl9wjladh\n",
            "Extracting page 12...\n",
            "Uploaded page 12 as: https://generativelanguage.googleapis.com/v1beta/files/k48vxt51dxz9 on attempt 1\n",
            "Deleted temporary file: files/k48vxt51dxz9\n",
            "Extracting page 13...\n",
            "Uploaded page 13 as: https://generativelanguage.googleapis.com/v1beta/files/pvbyh51phssf on attempt 1\n",
            "Error extracting text from page 13 on attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Retrying page 13 in 5 seconds...\n",
            "Uploaded page 13 as: https://generativelanguage.googleapis.com/v1beta/files/vo5084zs8w1r on attempt 2\n",
            "Deleted temporary file: files/vo5084zs8w1r\n",
            "Extracting page 14...\n",
            "Uploaded page 14 as: https://generativelanguage.googleapis.com/v1beta/files/ag3pyciog7re on attempt 1\n",
            "Deleted temporary file: files/ag3pyciog7re\n",
            "Extracting page 15...\n",
            "Uploaded page 15 as: https://generativelanguage.googleapis.com/v1beta/files/e6jrb5ovdrno on attempt 1\n",
            "Deleted temporary file: files/e6jrb5ovdrno\n",
            "Extracting page 16...\n",
            "Uploaded page 16 as: https://generativelanguage.googleapis.com/v1beta/files/fq94jjvf2rgk on attempt 1\n",
            "Deleted temporary file: files/fq94jjvf2rgk\n",
            "Extracting page 17...\n",
            "Uploaded page 17 as: https://generativelanguage.googleapis.com/v1beta/files/t3ln62g1muva on attempt 1\n",
            "Deleted temporary file: files/t3ln62g1muva\n",
            "Extracting page 18...\n",
            "Uploaded page 18 as: https://generativelanguage.googleapis.com/v1beta/files/s12dbwaejc94 on attempt 1\n",
            "Deleted temporary file: files/s12dbwaejc94\n",
            "Extracting page 19...\n",
            "Uploaded page 19 as: https://generativelanguage.googleapis.com/v1beta/files/ogjzk7zc98hl on attempt 1\n",
            "Deleted temporary file: files/ogjzk7zc98hl\n",
            "Extracting page 20...\n",
            "Uploaded page 20 as: https://generativelanguage.googleapis.com/v1beta/files/wad33v4p4uxi on attempt 1\n",
            "Deleted temporary file: files/wad33v4p4uxi\n",
            "Extracting page 21...\n",
            "Uploaded page 21 as: https://generativelanguage.googleapis.com/v1beta/files/i8r18x9ix1jb on attempt 1\n",
            "Deleted temporary file: files/i8r18x9ix1jb\n",
            "Extracting page 22...\n",
            "Uploaded page 22 as: https://generativelanguage.googleapis.com/v1beta/files/gbcwnq0e89zu on attempt 1\n",
            "Deleted temporary file: files/gbcwnq0e89zu\n",
            "Extracting page 23...\n",
            "Uploaded page 23 as: https://generativelanguage.googleapis.com/v1beta/files/pqeyfyeqttds on attempt 1\n",
            "Deleted temporary file: files/pqeyfyeqttds\n",
            "Extracting page 24...\n",
            "Uploaded page 24 as: https://generativelanguage.googleapis.com/v1beta/files/ehius2rchgbw on attempt 1\n",
            "Deleted temporary file: files/ehius2rchgbw\n",
            "Extracting page 25...\n",
            "Uploaded page 25 as: https://generativelanguage.googleapis.com/v1beta/files/125w2pm18mde on attempt 1\n",
            "Error extracting text from page 25 on attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Retrying page 25 in 5 seconds...\n",
            "Uploaded page 25 as: https://generativelanguage.googleapis.com/v1beta/files/xm2dnl1o8pk8 on attempt 2\n",
            "Deleted temporary file: files/xm2dnl1o8pk8\n",
            "Extracting page 26...\n",
            "Uploaded page 26 as: https://generativelanguage.googleapis.com/v1beta/files/47v4vajzg64w on attempt 1\n",
            "Deleted temporary file: files/47v4vajzg64w\n",
            "Extracting page 27...\n",
            "Uploaded page 27 as: https://generativelanguage.googleapis.com/v1beta/files/5jxzwbbpw8lz on attempt 1\n",
            "Error extracting text from page 27 on attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Retrying page 27 in 5 seconds...\n",
            "Uploaded page 27 as: https://generativelanguage.googleapis.com/v1beta/files/e6gj031a6u6t on attempt 2\n",
            "Deleted temporary file: files/e6gj031a6u6t\n",
            "Extracting page 28...\n",
            "Uploaded page 28 as: https://generativelanguage.googleapis.com/v1beta/files/h6ntv48rod2l on attempt 1\n",
            "Deleted temporary file: files/h6ntv48rod2l\n",
            "Extracting page 29...\n",
            "Uploaded page 29 as: https://generativelanguage.googleapis.com/v1beta/files/plvadix8t8yo on attempt 1\n",
            "Deleted temporary file: files/plvadix8t8yo\n",
            "Extracting page 30...\n",
            "Uploaded page 30 as: https://generativelanguage.googleapis.com/v1beta/files/y3v43ucadn3s on attempt 1\n",
            "Deleted temporary file: files/y3v43ucadn3s\n",
            "Extracting page 31...\n",
            "Uploaded page 31 as: https://generativelanguage.googleapis.com/v1beta/files/38v1ggekp3hd on attempt 1\n",
            "Deleted temporary file: files/38v1ggekp3hd\n",
            "Extracting page 32...\n",
            "Uploaded page 32 as: https://generativelanguage.googleapis.com/v1beta/files/72zxgjx7wzyw on attempt 1\n",
            "Deleted temporary file: files/72zxgjx7wzyw\n",
            "Extracting page 33...\n",
            "Uploaded page 33 as: https://generativelanguage.googleapis.com/v1beta/files/r4hdo73539xr on attempt 1\n",
            "Deleted temporary file: files/r4hdo73539xr\n",
            "Extracting page 34...\n",
            "Uploaded page 34 as: https://generativelanguage.googleapis.com/v1beta/files/b0osuftvpmmp on attempt 1\n",
            "Deleted temporary file: files/b0osuftvpmmp\n",
            "Extracting page 35...\n",
            "Uploaded page 35 as: https://generativelanguage.googleapis.com/v1beta/files/ibgn8gb7ae8j on attempt 1\n",
            "Deleted temporary file: files/ibgn8gb7ae8j\n",
            "Extracting page 36...\n",
            "Uploaded page 36 as: https://generativelanguage.googleapis.com/v1beta/files/klp57qgz81xt on attempt 1\n",
            "Deleted temporary file: files/klp57qgz81xt\n",
            "Extracting page 37...\n",
            "Uploaded page 37 as: https://generativelanguage.googleapis.com/v1beta/files/n1n0ya3nn160 on attempt 1\n",
            "Deleted temporary file: files/n1n0ya3nn160\n",
            "Extracting page 38...\n",
            "Uploaded page 38 as: https://generativelanguage.googleapis.com/v1beta/files/05gtugejpl96 on attempt 1\n",
            "Deleted temporary file: files/05gtugejpl96\n",
            "Extracting page 39...\n",
            "Uploaded page 39 as: https://generativelanguage.googleapis.com/v1beta/files/71nc00gq09uo on attempt 1\n",
            "Deleted temporary file: files/71nc00gq09uo\n",
            "Extracting page 40...\n",
            "Uploaded page 40 as: https://generativelanguage.googleapis.com/v1beta/files/2dlumglct57b on attempt 1\n",
            "Deleted temporary file: files/2dlumglct57b\n",
            "Extracting page 41...\n",
            "Uploaded page 41 as: https://generativelanguage.googleapis.com/v1beta/files/0sonmbnahelj on attempt 1\n",
            "Deleted temporary file: files/0sonmbnahelj\n",
            "Extracting page 42...\n",
            "Uploaded page 42 as: https://generativelanguage.googleapis.com/v1beta/files/jr8aysf5bpbo on attempt 1\n",
            "Error extracting text from page 42 on attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Retrying page 42 in 5 seconds...\n",
            "Uploaded page 42 as: https://generativelanguage.googleapis.com/v1beta/files/juy2ofmpwk87 on attempt 2\n",
            "Deleted temporary file: files/juy2ofmpwk87\n",
            "Extracting page 43...\n",
            "Uploaded page 43 as: https://generativelanguage.googleapis.com/v1beta/files/krn5bbhucfdg on attempt 1\n",
            "Deleted temporary file: files/krn5bbhucfdg\n",
            "Extracting page 44...\n",
            "Uploaded page 44 as: https://generativelanguage.googleapis.com/v1beta/files/fnivz5tc5zzy on attempt 1\n",
            "Deleted temporary file: files/fnivz5tc5zzy\n",
            "Extracting page 45...\n",
            "Uploaded page 45 as: https://generativelanguage.googleapis.com/v1beta/files/iw5j4a5rqs54 on attempt 1\n",
            "Deleted temporary file: files/iw5j4a5rqs54\n",
            "Extracting page 46...\n",
            "Uploaded page 46 as: https://generativelanguage.googleapis.com/v1beta/files/n41rf6a9gzmj on attempt 1\n",
            "Deleted temporary file: files/n41rf6a9gzmj\n",
            "Extracting page 47...\n",
            "Uploaded page 47 as: https://generativelanguage.googleapis.com/v1beta/files/kn7qar5xlmea on attempt 1\n",
            "Deleted temporary file: files/kn7qar5xlmea\n",
            "Extracting page 48...\n",
            "Uploaded page 48 as: https://generativelanguage.googleapis.com/v1beta/files/h2ogiftfwnuw on attempt 1\n",
            "Deleted temporary file: files/h2ogiftfwnuw\n",
            "Extracting page 49...\n",
            "Uploaded page 49 as: https://generativelanguage.googleapis.com/v1beta/files/rvkn0uxn6xd8 on attempt 1\n",
            "Deleted temporary file: files/rvkn0uxn6xd8\n",
            "Preprocessed text saved to /content/preprocessed_text.docx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dd331e42-ba3e-41d6-a1d4-a4c087b53161\", \"preprocessed_text.docx\", 78194)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessed Text Preview:\n",
            "\n",
            "\n",
            "--- Page 1 ---\n",
            "    বাংলা ১ম পত্র আলোচ্য বিষয় অপরিচিতা অনলাইন ব্যাচ সম্পর্কিত যেকোনো জিজ্ঞাসায় কল করো \n",
            "\n",
            "--- Page 2 ---\n",
            "  শিখনফল নিম্নবিত্ত ব্যক্তির হঠাৎ বিত্তশালী হয়ে ওঠার ফলে সমাজে পরিচয় সংকট সম্পর্কে ধারণা লাভ করবে।  তৎকালীন সমাজসভ্যতা ও মানবতার অবমাননা সম্পর্কে জানতে পারবে।  তৎকালীন সমাজের পণপ্রথার কুপ্রভাব সম্পর্কে জানতে পারবে।  তৎকালে সমাজে ভদ্রলোকের স্বভাববৈশিষ্ট্য সম্পর্কে জ্ঞানলাভ করবে।  নারী কোমল ঠিক কিন্তু দুর্বল নয় কল্যাণীর জীবনচরিত দ্বারা প্রতিষ্ঠিত এই সত্য অনুধাবন করতে পারবে।  মানুষ আশা নিয়ে বেঁচে থাকে অনুপমের দৃষ্টান্তে মানবজীবনের এই চিরন্তন সত্যদর্শন সম্পর্কে জ্ঞানলাভ করবে। প্রাকমূল্যায়ন ১। অনুপমের বাবা কী করে জীবিকা নির্বাহ করতেন ক ডাক্তারি খ ওকালতি গ মাস্টারি ২। মামাকে ভাগ্য দেবতার প্রধান এজেন্ট বলার কারণ তার ক প্রতিপত্তি খ প্রভাব ঘ ব্যবসা গ বিচক্ষণতা ঘ কূট বুদ্ধি নিচের অনুচ্ছেদটি পড়ে ৩ ও ৪ সংখ্যক প্রশ্নের উত্তর দাও। পিতৃহীন দীপুর চাচাই ছিলেন পরিবারের কর্তা। দীপু শিক্ষিত হলেও তার সিদ্ধান্ত নেওয়ার ক্ষমতা ছিল না। চাচা তার বিয়ের উদ্যোগ নিলেও যৌতুক নিয়ে বাড়াবা...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from docx import Document\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from pdf2image import convert_from_path\n",
        "import requests\n",
        "\n",
        "#configuring Gemini API\n",
        "def configure_gemini():\n",
        "    try:\n",
        "        api_key = userdata.get(\"GOOGLE_AOI_KEY_2\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key not found in Colab Secrets as 'GOOGLE_AOI_KEY_2'.\")\n",
        "        genai.configure(api_key=api_key)\n",
        "        model = genai.GenerativeModel('models/gemini-2.0-flash')  # Use Gemini 2.0 Flash\n",
        "        # Test API key with a simple request\n",
        "        model.generate_content(\"Test\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        print(\"Ensure the 'GOOGLE_AOI_KEY_2' secret is set in Colab Secrets: https://colab.research.google.com/drive/1...\")\n",
        "        return None\n",
        "\n",
        "#extracting Text from PDF\n",
        "def extract_text_from_page(pdf_path, page_num, model, max_retries=3, retry_delay=5):\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path, first_page=page_num, last_page=page_num, dpi=300)\n",
        "            if not images:\n",
        "                print(f\"Error: No image generated for page {page_num} on attempt {attempt}.\")\n",
        "                continue\n",
        "            temp_image_path = f\"/content/temp_page_{page_num}.png\"\n",
        "            images[0].save(temp_image_path, 'PNG')\n",
        "\n",
        "            sample_file = genai.upload_file(path=temp_image_path, display_name=f\"Page_{page_num}\")\n",
        "            print(f\"Uploaded page {page_num} as: {sample_file.uri} on attempt {attempt}\")\n",
        "\n",
        "            prompt = \"\"\"\n",
        "            Extract all text from the provided PDF page image, preserving the original Bengali script, sentence structure, and formatting as much as possible.\n",
        "            Include all questions, answers, passages, and vocabulary notes.\n",
        "            Ensure no content is omitted from the page.\n",
        "            Output the text in a clean, readable format without summarizing or modifying content.\n",
        "            \"\"\"\n",
        "            response = model.generate_content([sample_file, prompt])\n",
        "\n",
        "            genai.delete_file(sample_file.name)\n",
        "            print(f\"Deleted temporary file: {sample_file.name}\")\n",
        "\n",
        "            os.remove(temp_image_path)\n",
        "\n",
        "            text = response.text if response.text else None\n",
        "            if text:\n",
        "                return text\n",
        "            print(f\"Warning: No text extracted from page {page_num} on attempt {attempt}.\")\n",
        "        except (requests.exceptions.ConnectionError, Exception) as e:\n",
        "            print(f\"Error extracting text from page {page_num} on attempt {attempt}: {e}\")\n",
        "            if attempt < max_retries:\n",
        "                print(f\"Retrying page {page_num} in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            continue\n",
        "        finally:\n",
        "            if os.path.exists(temp_image_path):\n",
        "                os.remove(temp_image_path)\n",
        "\n",
        "    print(f\"Failed to extract text from page {page_num} after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    text = re.sub(r'[^\\u0980-\\u09FF\\s।]', '', text)\n",
        "    text = text.replace('া ু', 'ৌ').replace('ি ী', 'ী').replace('ু ু', 'ূ')\n",
        "    text = re.sub(r'অনলাইন ব্যাচ বাংলা ইংরেজি আইসিটি\\s*', '', text)\n",
        "    return text\n",
        "\n",
        "def save_to_word(text, output_path=\"/content/preprocessed_text.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_paragraph(text)\n",
        "    doc.save(output_path)\n",
        "    print(f\"Preprocessed text saved to {output_path}\")\n",
        "    files.download(output_path)\n",
        "\n",
        "def preprocess_pdf(pdf_path=\"/content/HSC26-Bangla1st-Paper.pdf\"):\n",
        "    model = configure_gemini()\n",
        "    if not model:\n",
        "        print(\"Error: Failed to initialize Gemini model. Check your API key in Colab Secrets.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=100)\n",
        "        total_pages = len(images)\n",
        "        print(f\"Total pages in PDF: {total_pages}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error determining page count: {e}\")\n",
        "        return\n",
        "\n",
        "    all_text = \"\"\n",
        "    for page_num in range(1, total_pages + 1):\n",
        "        print(f\"Extracting page {page_num}...\")\n",
        "        page_text = extract_text_from_page(pdf_path, page_num, model)\n",
        "        if page_text:\n",
        "            cleaned_text = clean_text(page_text)\n",
        "            all_text += f\"\\n\\n--- Page {page_num} ---\\n{cleaned_text}\"\n",
        "        else:\n",
        "            print(f\"Warning: No text extracted from page {page_num} after retries.\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    if not all_text.strip():\n",
        "        print(\"Error: No text extracted from any page. Ensure the PDF is valid.\")\n",
        "        return\n",
        "    save_to_word(all_text)\n",
        "\n",
        "    print(\"\\nPreprocessed Text Preview:\")\n",
        "    print(all_text[:1000] + \"...\" if len(all_text) > 1000 else all_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    preprocess_pdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG Application complete code"
      ],
      "metadata": {
        "id": "eXBQtacu35rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries for Colab\n",
        "!pip install python-docx docx2txt nltk indic-nlp-library sentence-transformers pinecone openai flask pyngrok -q\n",
        "!pip install git+https://github.com/csebuetnlp/normalizer -q\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import csv\n",
        "from docx2txt import docx2txt\n",
        "from normalizer import normalize\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from openai import OpenAI\n",
        "from collections import deque\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import json\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "import socket\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "conversation_history = deque(maxlen=10)  # Stores last 10 queries\n",
        "model = None\n",
        "index = None\n",
        "chunks = None\n",
        "openai_api_key = None\n",
        "\n",
        "def find_free_port():\n",
        "    \"\"\"\n",
        "    Find an available port for the Flask server.\n",
        "    \"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "def terminate_port_processes(port):\n",
        "    \"\"\"\n",
        "    Terminate any processes using the specified port using lsof and kill.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(['lsof', '-i', f':{port}'], capture_output=True, text=True)\n",
        "        lines = result.stdout.splitlines()\n",
        "        for line in lines[1:]:  # Skip header\n",
        "            parts = line.split()\n",
        "            if len(parts) > 1:\n",
        "                pid = parts[1]\n",
        "                subprocess.run(['kill', '-9', pid])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def load_and_preprocess_docx(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess text from a .docx file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = docx2txt.process(file_path)\n",
        "        text = re.sub(r'--- Page \\d+ ---', '', text)\n",
        "        text = re.sub(r'অনলাইন ব্যাচ সম্পর্কিত যেকোনো জিজ্ঞাসায় কল করো', '', text)\n",
        "        text = re.sub(r'বাংলা ১ম পত্র আলোচ্য বিষয় অপরিচিতা', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "        return normalize(text)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to load document: {str(e)}\")\n",
        "\n",
        "def chunk_text(text, language=\"bn\"):\n",
        "    \"\"\"\n",
        "    Chunk text into sentences for semantic retrieval.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if language == \"bn\":\n",
        "            sentences = sentence_tokenize.sentence_split(text, lang=\"bn\")\n",
        "        else:\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "        return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to chunk text: {str(e)}\")\n",
        "\n",
        "def save_chunks(chunks, output_file=\"sentence_chunks.csv\"):\n",
        "    \"\"\"\n",
        "    Save chunks to a CSV file.\n",
        "    \"\"\"\n",
        "    if output_file.endswith(\".csv\"):\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"Index\", \"Text\"])\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                writer.writerow([i, chunk])\n",
        "    else:\n",
        "        raise ValueError(\"Output file must have .csv extension\")\n",
        "\n",
        "def initialize_vector_store(chunks, model_name=\"intfloat/multilingual-e5-large\", api_key=None, index_name=\"rag-index\", batch_size=100):\n",
        "    \"\"\"\n",
        "    Vectorize chunks and store in a Pinecone index with chunk indices.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = SentenceTransformer(model_name)\n",
        "        embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "        dimension = embeddings.shape[1]\n",
        "        if dimension != 1024:\n",
        "            raise ValueError(f\"Embedding dimension {dimension} does not match expected 1024 for rag-index\")\n",
        "\n",
        "        pc = Pinecone(api_key=api_key)\n",
        "        if index_name not in pc.list_indexes().names():\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=1024,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "        index = pc.Index(index_name)\n",
        "\n",
        "        vectors = [(f\"doc_{i}\", embedding, {\"text\": chunk, \"index\": i}) for i, (embedding, chunk) in enumerate(zip(embeddings.tolist(), chunks))]\n",
        "        for i in range(0, len(vectors), batch_size):\n",
        "            batch = vectors[i:i + batch_size]\n",
        "            index.upsert(vectors=batch, namespace=\"\")\n",
        "\n",
        "        stats = index.describe_index_stats()\n",
        "        if stats['total_vector_count'] != len(chunks):\n",
        "            raise Exception(f\"Vector count mismatch: {stats['total_vector_count']} in Pinecone, {len(chunks)} expected\")\n",
        "\n",
        "        return model, index, chunks\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to initialize vector store: {str(e)}\")\n",
        "\n",
        "def retrieve_similar_chunks(query, model, index, top_k=10):\n",
        "    \"\"\"\n",
        "    Retrieve similar chunks from Pinecone index for a given query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query = normalize(query)\n",
        "        query_embedding = model.encode([query], convert_to_numpy=True)[0]\n",
        "        results = index.query(\n",
        "            vector=query_embedding.tolist(),\n",
        "            top_k=top_k,\n",
        "            include_metadata=True,\n",
        "            namespace=\"\"\n",
        "        )\n",
        "        return [(match['metadata']['text'], match['score'], match['metadata']['index']) for match in results['matches']]\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to retrieve chunks: {str(e)}\")\n",
        "\n",
        "def generate_answer(query, retrieved_chunks, conversation_history, openai_api_key, model_name=\"gpt-4o\"):\n",
        "    \"\"\"\n",
        "    Generate an answer using GPT-4o with retrieved chunks and conversation history.\n",
        "    \"\"\"\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "    context = \"\\n\".join([f\"Chunk {i+1} (Index {chunk[2]}): {chunk[0]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    history_context = \"\"\n",
        "    if conversation_history:\n",
        "        history_context = \"Recent Conversation History (most recent first):\\n\"\n",
        "        for h_query, h_answer in reversed(list(conversation_history)):\n",
        "            history_context += f\"User: {h_query}\\nAnswer: {h_answer}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful teaching assistant that answers questions based on provided document context and the most recent conversation history. Ensure the answer is clear and relevant by focusing on:\n",
        "\n",
        "1. Context Usage: Resolve pronouns and references by using the most recent relevant entity mentioned.\n",
        "2. Query Resolution: Answer based on the most recent query-response pair first, followed by the document context if necessary.\n",
        "3. Relevance: If context is insufficient, refer to your knowledge base while ensuring brevity and clarity. Avoid irrelevant details.\n",
        "\n",
        "Please answer in the same language as the query. Ensure that responses are concise and directly related to the asked question, grounded in the retrieved document content.\n",
        "\n",
        "Document Context:\n",
        "{context}\n",
        "\n",
        "{history_context}\n",
        "\n",
        "Current Query: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant with accurate context retention.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to generate answer with GPT-4o: {str(e)}\")\n",
        "\n",
        "# REST API Endpoint\n",
        "@app.route('/api/query', methods=['POST'])\n",
        "def handle_query():\n",
        "    \"\"\"\n",
        "    Endpoint to handle user queries and return generated answers in plain text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not all([model, index, openai_api_key]):\n",
        "            return jsonify({\"error\": \"RAG system not initialized properly\"}), 500\n",
        "\n",
        "        data = request.get_json()\n",
        "        if not data or 'query' not in data:\n",
        "            return jsonify({\"error\": \"Query is required in JSON payload\"}), 400\n",
        "\n",
        "        query = data['query'].strip()\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Query cannot be empty\"}), 400\n",
        "\n",
        "        retrieved_chunks = retrieve_similar_chunks(query, model, index)\n",
        "        answer = generate_answer(query, retrieved_chunks, conversation_history, openai_api_key)\n",
        "        conversation_history.append((query, answer))\n",
        "        return jsonify({\"answer\": json.loads(json.dumps(answer))})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Internal server error: {str(e)}\"}), 500\n",
        "\n",
        "def evaluate_rag(query, expected_answer, retrieved_chunks):\n",
        "    \"\"\"\n",
        "    Evaluate RAG system for a query using groundedness and relevance.\n",
        "    \"\"\"\n",
        "    avg_score = sum(score for _, score, _ in retrieved_chunks) / len(retrieved_chunks) if retrieved_chunks else 0\n",
        "    grounded = any(expected_answer.lower() in chunk.lower() for chunk, _, _ in retrieved_chunks)\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"avg_similarity\": round(avg_score, 3),\n",
        "        \"grounded\": \"SUPPORTED\" if grounded else \"NOT SUPPORTED\"\n",
        "    }\n",
        "\n",
        "def test_query(query, expected_answer, api_url):\n",
        "    \"\"\"\n",
        "    Send a query to the API and print the answer, expected answer, and evaluation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{api_url}/api/query\",\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            json={\"query\": query}\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        if \"answer\" in result:\n",
        "            retrieved_chunks = retrieve_similar_chunks(query, model, index)\n",
        "            eval_result = evaluate_rag(query, expected_answer, retrieved_chunks)\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Actual Answer: {result['answer']}\")\n",
        "            print(f\"Expected Answer: {expected_answer}\")\n",
        "            print(f\"Evaluation: Groundedness={eval_result['grounded']}, Relevance (Avg Cosine Similarity)={eval_result['avg_similarity']}\")\n",
        "            print(\"-\" * 50)\n",
        "        else:\n",
        "            print(f\"Error for query '{query}': {result.get('error', 'Unknown error')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing query '{query}': {str(e)}\")\n",
        "\n",
        "def initialize_rag_system():\n",
        "    global model, index, chunks, openai_api_key\n",
        "    file_path = \"preprocessed_text.docx\"\n",
        "    pinecone_api_key = userdata.get(\"Pinecone_API_KEY\")\n",
        "    openai_api_key = userdata.get(\"my_GPT_key_2\")\n",
        "    ngrok_auth_token = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
        "\n",
        "    if not pinecone_api_key or not openai_api_key or not ngrok_auth_token:\n",
        "        raise ValueError(\"API keys must be set in Colab Secrets\")\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"'{file_path}' not found. Upload the file.\")\n",
        "\n",
        "    normalized_text = load_and_preprocess_docx(file_path)\n",
        "    chunks = chunk_text(normalized_text, language=\"bn\")\n",
        "    save_chunks(chunks)\n",
        "    model, index, chunks = initialize_vector_store(chunks, api_key=pinecone_api_key)\n",
        "\n",
        "def run_flask(port):\n",
        "    app.run(host='0.0.0.0', port=port)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "        terminate_port_processes(5000)\n",
        "        port = find_free_port()\n",
        "        initialize_rag_system()\n",
        "        ngrok.set_auth_token(userdata.get(\"NGROK_AUTH_TOKEN\"))\n",
        "        public_url = ngrok.connect(port)\n",
        "        api_url = public_url.public_url\n",
        "        print(f\"Public URL for API: {api_url}\")\n",
        "\n",
        "        # Start Flask server in a thread\n",
        "        flask_thread = threading.Thread(target=run_flask, args=(port,), daemon=True)\n",
        "        flask_thread.start()\n",
        "        time.sleep(2)  # Wait for Flask server to start\n",
        "\n",
        "        # Verify server is running\n",
        "        try:\n",
        "            response = requests.get(f\"http://localhost:{port}/api/query\", timeout=5)\n",
        "            if response.status_code != 405:  # Expect 405 (Method Not Allowed) for GET\n",
        "                raise Exception(\"Flask server not responding as expected\")\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            raise Exception(f\"Flask server failed to start on port {port}\")\n",
        "\n",
        "        # Corrected sample queries\n",
        "        sample_evals = [\n",
        "            {\"query\": \"অনুপমের ভাষায় সুপরুষ কাকে বলা হয়েছে?\", \"expected\": \"শম্ভুনাথ\"},\n",
        "            {\"query\": \"কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\", \"expected\": \"মামাকে\"},\n",
        "            {\"query\": \"Who is the main character in the story?\", \"expected\": \"অনুপম\"}\n",
        "        ]\n",
        "        for sample in sample_evals:\n",
        "            test_query(sample[\"query\"], sample[\"expected\"], api_url=api_url)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bad83baa0392415aafaa8d11275654c7",
            "40d1a2f40c844d2f8212e4a8d897e7bf",
            "91cf28755b7e49ec8ed95752b5f90067",
            "de77cbcc8d8448dcb0c3a0504621e56f",
            "54c3baf6771f4e3a8814d890c3c86bea",
            "8b7068f4af43441e9201dd04c7d5f127",
            "3947cc0802be42f4967d62c82cadbfae",
            "cf2e5a7c8312492c8f58522f941c3f00",
            "7a23bdde25b84792a415a894653c605b",
            "5c452e77d5b643769d9206c3aaad104b",
            "3a33c915132f47168c7cf1738dde2355",
            "f974d51729e14002a890815d078e3daa",
            "43d8c1651be149afac7dcaf16ffd06e2",
            "169b4d2c53eb4b9ca21d0cd2cae9b6fd",
            "1e89e2036d764600923e260510bb38e9",
            "b022d1bb421f42b7becccccb49c26e0f",
            "32419143835d4058b47a2835fea0292f",
            "58607c09e69a41a5bead73d1d6247da3",
            "119c45cd8fdf4291bff131be6c76ba6a",
            "a05b6036d5324b098bab3a21ca0aee71",
            "db70a93e59ca4fb6952c1a556ff5ec0d",
            "8cfd83bef2ff4e85babaabc29a70ad9e",
            "1a9efae5a3ef4f08957218d26a19769b",
            "1e15770a0f7847dcbde2d12505d830a5",
            "e7175f45e57a4dd79d0580e237e405d6",
            "f1b7c6121e6e48eeb80f477dea16fad2",
            "c0d8262134164cd897da620e6b626a4b",
            "6ecf252140084a6bbfa50e80818ed59a",
            "d5881b3145be4d27ae3bf34b9620ac54",
            "eed92473ad4644ceb58e73bd7847f26d",
            "cf730df77938495fb1c7b8846e1937bc",
            "63e2715e67394e499bf977561fc73ba8",
            "43ee552b82044b06826779a283a8ca62",
            "71c2ec536ebe48f18a5d8e6ff8880833",
            "e2a5eafd3df3460dac1ee9f3070d3094",
            "e66688bb52c84ec9b106521c8cfd051a",
            "27382e9890df4df392fca9d304d900e8",
            "ad1d58f54611424c8cceeaf007747f99",
            "58ad387e739144d584e9a1cff4553b05",
            "4a278188c94245a1980cb410827bd54a",
            "7a254316dd4f485bac5c516326e78d1a",
            "3ba0e31346144812943bee4b3e3f229b",
            "09bbf27de4fd44d7b7daac20ab9d691d",
            "83734fbc67154ae4b108bd1b0439e26c",
            "c9e4b027b4494a22ba49af8f8c19f32e",
            "5a53d44d2d114b4bb29de7fc0eddae1d",
            "e67c43dcc6e24e79b1742a1b5c2886b8",
            "157bbf640a6b45f1a19556553606de60",
            "006eb1c07c0549438abcf29368f5a611",
            "e52956bb55824b1a895a032129392fd8",
            "54f1ac1dfb774603ac934ee46b58bac9",
            "bd175ae7a4a64e3fb84347c384fe736f",
            "5784a27c21ba404fada490c7bf2294a9",
            "1e06b6fb5ef1432e9c427326941d2d60",
            "1707c31708334a3092fad01105736cb2",
            "4fa9ba75ec7c45e286ec799a208b3eb5",
            "1e67b5b317a448ce98f1955bb42b7edf",
            "551c45b536544b0da9e885636aedff09",
            "5ba73c78693241e594dfc3930abe9a07",
            "cd3baedff800499b8cab4ca10746fd87",
            "0d86f3ff45e14845b26299e9a69cb60e",
            "94e530a2978242249aac018818f9f9f3",
            "f61f34e87dff48d5bf7262f1436e370b",
            "f5a3a056c46341c9ad48fde93040482c",
            "c74e0f50af2743ac8fc164f0bcb8ea45",
            "50649bb303844200b3b1abf3cf10242d",
            "bb9eddd0bec949b089789bbf59d41b29",
            "483a4fc18be64e94848290792500f4d4",
            "9f452db2cf354c9aab6319c0e8350680",
            "2e469cbc53db41068c0c3a8011b02600",
            "04f7db69a66a41b7ad08151da99e52e0",
            "1b93e3a5a9ce43a2b41d48cba637280c",
            "df87d2659593479489f34d0e3420c0dd",
            "6f59c80688a3486ab6857ea6b53040a6",
            "dc6622e43e3b49f888bde27cf1eb3bb5",
            "40cd4075459c44afbdea4c8b81f62018",
            "f888a9b81e154891b9baaeff442898e4",
            "c334bd222fdf4566b7da620b6f9a9b20",
            "b8cfac8cbe76414aa13a358d52654750",
            "b09cb8e2fb184015be7d9c880f7b4068",
            "4c60f1ea681f479ab85c8e49e095ff5b",
            "771409c011164812a93e75f276a728b0",
            "d6e63d58049d4260b0e00717ef9d7a6e",
            "4bbc19039e684906a14fc498c979112d",
            "9fe2d649231845a7bf3a707d4ed1c428",
            "2d3ee15c052845159c35d5cc73520ffa",
            "16bf57a7f1db4d1fb7f596f1ef51a69d",
            "e74ebaadf07d4f7786ed623c717be38f",
            "538f3aee8d8f41bc9e6a44cb42740b07",
            "89dd29a4e27c47fc86006791389ed11f",
            "79855959337d4d6a96bebd3aeb4401ce",
            "1036834f43be4f47a5c37b16e04d64a5",
            "6aaf6660d4be4372b93eb6bb8431f286",
            "302007fef7a140ae87f0605b5bc1ff1b",
            "555b6843e6db454abaef60a1c545e527",
            "b6464c6225ae4e9a91c081f16caad79a",
            "07dc0597d09242e794303b02cebbd2b4",
            "180afdf4070b4f288edb34d1d28c59b8",
            "e9be1d239a8e4d6db53837dad7922f48",
            "e48963be29714f7fb42a657ae053e8e0",
            "eb7bf4ef01234dd9a926d222ed3e686f",
            "b500db6e79bc4621aad39d917444eb70",
            "25a610e94c144af2a382c00eb610d27f",
            "94802d994ac4415daec3cc6007f9cfae",
            "567e6ee82cc34a4bbc5233c3061813d8",
            "1f5730dafe374c01986db2776a61a2d7",
            "ce08fd92a72c450cad6393d6e740a576",
            "d5c2608528154ddf863fcdc886437e05",
            "0ddbd7409d28458f9fad18aa9bbdf64f",
            "4aa4d087f6fa4725877d3a5a5664a693",
            "164c34bcba3447baa8d4be009fffaf64",
            "3c35fc02cce543f0a87093271972b441",
            "b2ec113bcfbb4dfeaeaac5d8c44c1be0",
            "959f03426eef48b0aa5e6f5ca5fd6a63",
            "3d711d78d77847cc85d78d74295b7acf",
            "801a510ec3b74cc7963811c28e9dfe05",
            "d4109c5f72534b579e906274d7bf871e",
            "75058a27aae5425ababfbad2ea277ad9",
            "1ed4a160fbe24e28b53e66590cce42ce",
            "d58c0c0bcd4d445396cd154564814b6e",
            "ec4623c8a30c46e9b267f89ad8a1dea5"
          ]
        },
        "id": "VGDoDozwHRNA",
        "outputId": "cfea4d1f-afe9-48fc-820b-c3d73e8a2526",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/587.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m553.0/587.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m882.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bad83baa0392415aafaa8d11275654c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f974d51729e14002a890815d078e3daa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a9efae5a3ef4f08957218d26a19769b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71c2ec536ebe48f18a5d8e6ff8880833"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9e4b027b4494a22ba49af8f8c19f32e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fa9ba75ec7c45e286ec799a208b3eb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb9eddd0bec949b089789bbf59d41b29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c334bd222fdf4566b7da620b6f9a9b20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "538f3aee8d8f41bc9e6a44cb42740b07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e48963be29714f7fb42a657ae053e8e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "164c34bcba3447baa8d4be009fffaf64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL for API: https://ff1aefcc80d7.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:56667\n",
            " * Running on http://172.28.0.12:56667\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jul/2025 17:45:53] \"\u001b[31m\u001b[1mGET /api/query HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jul/2025 17:45:55] \"POST /api/query HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: অনুপমের ভাষায় সুপরুষ কাকে বলা হয়েছে?\n",
            "Actual Answer: অনুপমের ভাষায় সুপুরুষ বলা হয়েছে শম্ভুনাথকে।\n",
            "Expected Answer: শম্ভুনাথ\n",
            "Evaluation: Groundedness=SUPPORTED, Relevance (Avg Cosine Similarity)=0.819\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Jul/2025 17:45:56] \"POST /api/query HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\n",
            "Actual Answer: অনুপমের ভাগ্য দেবতা বলে তার মামাকে উল্লেখ করা হয়েছে।\n",
            "Expected Answer: মামাকে\n",
            "Evaluation: Groundedness=SUPPORTED, Relevance (Avg Cosine Similarity)=0.809\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Jul/2025 17:46:00] \"POST /api/query HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Who is the main character in the story?\n",
            "Actual Answer: The main character in the story \"অপরিচিতা\" is অনুপম.\n",
            "Expected Answer: অনুপম\n",
            "Evaluation: Groundedness=SUPPORTED, Relevance (Avg Cosine Similarity)=0.795\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}