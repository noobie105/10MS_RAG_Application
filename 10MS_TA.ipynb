{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXdB4Ony7jqVbVR8QTCziL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobie105/10MS_RAG_Application/blob/main/10MS_TA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting PDF to DOCX using GEMINI"
      ],
      "metadata": {
        "id": "RPZEJUUO3sRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image python-docx\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "id": "w0UrTmQD3phS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from docx import Document\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from pdf2image import convert_from_path\n",
        "import requests\n",
        "\n",
        "#configuring Gemini API\n",
        "def configure_gemini():\n",
        "    try:\n",
        "        api_key = userdata.get(\"GOOGLE_AOI_KEY_2\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key not found in Colab Secrets as 'GOOGLE_AOI_KEY_2'.\")\n",
        "        genai.configure(api_key=api_key)\n",
        "        model = genai.GenerativeModel('models/gemini-2.0-flash')  # Use Gemini 2.0 Flash\n",
        "        # Test API key with a simple request\n",
        "        model.generate_content(\"Test\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        print(\"Ensure the 'GOOGLE_AOI_KEY_2' secret is set in Colab Secrets: https://colab.research.google.com/drive/1...\")\n",
        "        return None\n",
        "\n",
        "#extracting Text from PDF\n",
        "def extract_text_from_page(pdf_path, page_num, model, max_retries=3, retry_delay=5):\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path, first_page=page_num, last_page=page_num, dpi=300)\n",
        "            if not images:\n",
        "                print(f\"Error: No image generated for page {page_num} on attempt {attempt}.\")\n",
        "                continue\n",
        "            temp_image_path = f\"/content/temp_page_{page_num}.png\"\n",
        "            images[0].save(temp_image_path, 'PNG')\n",
        "\n",
        "            sample_file = genai.upload_file(path=temp_image_path, display_name=f\"Page_{page_num}\")\n",
        "            print(f\"Uploaded page {page_num} as: {sample_file.uri} on attempt {attempt}\")\n",
        "\n",
        "            prompt = \"\"\"\n",
        "            Extract all text from the provided PDF page image, preserving the original Bengali script, sentence structure, and formatting as much as possible.\n",
        "            Include all questions, answers, passages, and vocabulary notes.\n",
        "            Ensure no content is omitted from the page.\n",
        "            Output the text in a clean, readable format without summarizing or modifying content.\n",
        "            \"\"\"\n",
        "            response = model.generate_content([sample_file, prompt])\n",
        "\n",
        "            genai.delete_file(sample_file.name)\n",
        "            print(f\"Deleted temporary file: {sample_file.name}\")\n",
        "\n",
        "            os.remove(temp_image_path)\n",
        "\n",
        "            text = response.text if response.text else None\n",
        "            if text:\n",
        "                return text\n",
        "            print(f\"Warning: No text extracted from page {page_num} on attempt {attempt}.\")\n",
        "        except (requests.exceptions.ConnectionError, Exception) as e:\n",
        "            print(f\"Error extracting text from page {page_num} on attempt {attempt}: {e}\")\n",
        "            if attempt < max_retries:\n",
        "                print(f\"Retrying page {page_num} in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            continue\n",
        "        finally:\n",
        "            if os.path.exists(temp_image_path):\n",
        "                os.remove(temp_image_path)\n",
        "\n",
        "    print(f\"Failed to extract text from page {page_num} after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    text = re.sub(r'[^\\u0980-\\u09FF\\s।]', '', text)\n",
        "    text = text.replace('া ু', 'ৌ').replace('ি ী', 'ী').replace('ু ু', 'ূ')\n",
        "    text = re.sub(r'অনলাইন ব্যাচ বাংলা ইংরেজি আইসিটি\\s*', '', text)\n",
        "    return text\n",
        "\n",
        "def save_to_word(text, output_path=\"/content/preprocessed_text.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_paragraph(text)\n",
        "    doc.save(output_path)\n",
        "    print(f\"Preprocessed text saved to {output_path}\")\n",
        "    files.download(output_path)\n",
        "\n",
        "def preprocess_pdf(pdf_path=\"/content/HSC26-Bangla1st-Paper.pdf\"):\n",
        "    model = configure_gemini()\n",
        "    if not model:\n",
        "        print(\"Error: Failed to initialize Gemini model. Check your API key in Colab Secrets.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=100)\n",
        "        total_pages = len(images)\n",
        "        print(f\"Total pages in PDF: {total_pages}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error determining page count: {e}\")\n",
        "        return\n",
        "\n",
        "    all_text = \"\"\n",
        "    for page_num in range(1, total_pages + 1):\n",
        "        print(f\"Extracting page {page_num}...\")\n",
        "        page_text = extract_text_from_page(pdf_path, page_num, model)\n",
        "        if page_text:\n",
        "            cleaned_text = clean_text(page_text)\n",
        "            all_text += f\"\\n\\n--- Page {page_num} ---\\n{cleaned_text}\"\n",
        "        else:\n",
        "            print(f\"Warning: No text extracted from page {page_num} after retries.\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    if not all_text.strip():\n",
        "        print(\"Error: No text extracted from any page. Ensure the PDF is valid.\")\n",
        "        return\n",
        "    save_to_word(all_text)\n",
        "\n",
        "    print(\"\\nPreprocessed Text Preview:\")\n",
        "    print(all_text[:1000] + \"...\" if len(all_text) > 1000 else all_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    preprocess_pdf()"
      ],
      "metadata": {
        "id": "4flSmjoD3bmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG Application complete code"
      ],
      "metadata": {
        "id": "eXBQtacu35rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx docx2txt nltk indic-nlp-library sentence-transformers pinecone openai flask pyngrok -q\n",
        "!pip install git+https://github.com/csebuetnlp/normalizer -q\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import csv\n",
        "from docx2txt import docx2txt\n",
        "from normalizer import normalize\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from openai import OpenAI\n",
        "from collections import deque\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import json\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "import socket\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "nltk.download('punkt')\n",
        "app = Flask(__name__)\n",
        "\n",
        "conversation_history = deque(maxlen=10)\n",
        "model = None\n",
        "index = None\n",
        "chunks = None\n",
        "openai_api_key = None\n",
        "\n",
        "def find_free_port():\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "def terminate_port_processes(port):\n",
        "    try:\n",
        "        result = subprocess.run(['lsof', '-i', f':{port}'], capture_output=True, text=True)\n",
        "        lines = result.stdout.splitlines()\n",
        "        for line in lines[1:]:\n",
        "            parts = line.split()\n",
        "            if len(parts) > 1:\n",
        "                pid = parts[1]\n",
        "                subprocess.run(['kill', '-9', pid])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def load_and_preprocess_docx(file_path):\n",
        "    try:\n",
        "        text = docx2txt.process(file_path)\n",
        "        text = re.sub(r'--- Page \\d+ ---', '', text)\n",
        "        text = re.sub(r'অনলাইন ব্যাচ সম্পর্কিত যেকোনো জিজ্ঞাসায় কল করো', '', text)\n",
        "        text = re.sub(r'বাংলা ১ম পত্র আলোচ্য বিষয় অপরিচিতা', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "        return normalize(text)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to load document: {str(e)}\")\n",
        "\n",
        "def chunk_text(text, language=\"bn\"):\n",
        "    try:\n",
        "        if language == \"bn\":\n",
        "            sentences = sentence_tokenize.sentence_split(text, lang=\"bn\")\n",
        "        else:\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "        return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to chunk text: {str(e)}\")\n",
        "\n",
        "def save_chunks(chunks, output_file=\"sentence_chunks.csv\"):\n",
        "    if output_file.endswith(\".csv\"):\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"Index\", \"Text\"])\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                writer.writerow([i, chunk])\n",
        "    else:\n",
        "        raise ValueError(\"Output file must have .csv extension\")\n",
        "\n",
        "def initialize_vector_store(chunks, model_name=\"intfloat/multilingual-e5-large\", api_key=None, index_name=\"rag-index\", batch_size=100):\n",
        "    try:\n",
        "        model = SentenceTransformer(model_name)\n",
        "        embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "        dimension = embeddings.shape[1]\n",
        "        if dimension != 1024:\n",
        "            raise ValueError(f\"Embedding dimension {dimension} does not match expected 1024 for rag-index\")\n",
        "\n",
        "        pc = Pinecone(api_key=api_key)\n",
        "        if index_name not in pc.list_indexes().names():\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=1024,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "        index = pc.Index(index_name)\n",
        "\n",
        "        vectors = [(f\"doc_{i}\", embedding, {\"text\": chunk, \"index\": i}) for i, (embedding, chunk) in enumerate(zip(embeddings.tolist(), chunks))]\n",
        "        for i in range(0, len(vectors), batch_size):\n",
        "            batch = vectors[i:i + batch_size]\n",
        "            index.upsert(vectors=batch, namespace=\"\")\n",
        "\n",
        "        stats = index.describe_index_stats()\n",
        "        if stats['total_vector_count'] != len(chunks):\n",
        "            raise Exception(f\"Vector count mismatch: {stats['total_vector_count']} in Pinecone, {len(chunks)} expected\")\n",
        "\n",
        "        return model, index, chunks\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to initialize vector store: {str(e)}\")\n",
        "\n",
        "def retrieve_similar_chunks(query, model, index, top_k=10):\n",
        "    try:\n",
        "        query = normalize(query)\n",
        "        query_embedding = model.encode([query], convert_to_numpy=True)[0]\n",
        "        results = index.query(\n",
        "            vector=query_embedding.tolist(),\n",
        "            top_k=top_k,\n",
        "            include_metadata=True,\n",
        "            namespace=\"\"\n",
        "        )\n",
        "        return [(match['metadata']['text'], match['score'], match['metadata']['index']) for match in results['matches']]\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to retrieve chunks: {str(e)}\")\n",
        "\n",
        "def generate_answer(query, retrieved_chunks, conversation_history, openai_api_key, model_name=\"gpt-4o\"):\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "    context = \"\\n\".join([f\"Chunk {i+1} (Index {chunk[2]}): {chunk[0]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    history_context = \"\"\n",
        "    if conversation_history:\n",
        "        history_context = \"Recent Conversation History (most recent first):\\n\"\n",
        "        for h_query, h_answer in reversed(list(conversation_history)):\n",
        "            history_context += f\"User: {h_query}\\nAnswer: {h_answer}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful teaching assistant that answers questions based on provided document context and the most recent conversation history. Ensure the answer is clear and relevant by focusing on:\n",
        "\n",
        "1. Context Usage: Resolve pronouns and references by using the most recent relevant entity mentioned.\n",
        "2. Query Resolution: Answer based on the most recent query-response pair first, followed by the document context if necessary.\n",
        "3. Relevance: If context is insufficient, refer to your knowledge base while ensuring brevity and clarity. Avoid irrelevant details.\n",
        "\n",
        "Please answer in the same language as the query. Ensure that responses are concise and directly related to the asked question, grounded in the retrieved document content.\n",
        "\n",
        "Document Context:\n",
        "{context}\n",
        "\n",
        "{history_context}\n",
        "\n",
        "Current Query: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant with accurate context retention.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to generate answer with GPT-4o: {str(e)}\")\n",
        "\n",
        "#REST API Endpoint\n",
        "@app.route('/api/query', methods=['POST'])\n",
        "def handle_query():\n",
        "    try:\n",
        "        if not all([model, index, openai_api_key]):\n",
        "            return jsonify({\"error\": \"RAG system not initialized properly\"}), 500\n",
        "\n",
        "        data = request.get_json()\n",
        "        if not data or 'query' not in data:\n",
        "            return jsonify({\"error\": \"Query is required in JSON payload\"}), 400\n",
        "\n",
        "        query = data['query'].strip()\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Query cannot be empty\"}), 400\n",
        "\n",
        "        retrieved_chunks = retrieve_similar_chunks(query, model, index)\n",
        "        answer = generate_answer(query, retrieved_chunks, conversation_history, openai_api_key)\n",
        "        conversation_history.append((query, answer))\n",
        "        return jsonify({\"answer\": json.loads(json.dumps(answer))})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Internal server error: {str(e)}\"}), 500\n",
        "\n",
        "#Evaluation of the system\n",
        "def evaluate_rag(query, expected_answer, retrieved_chunks):\n",
        "    avg_score = sum(score for _, score, _ in retrieved_chunks) / len(retrieved_chunks) if retrieved_chunks else 0\n",
        "    grounded = any(expected_answer.lower() in chunk.lower() for chunk, _, _ in retrieved_chunks)\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"avg_similarity\": round(avg_score, 3),\n",
        "        \"grounded\": \"SUPPORTED\" if grounded else \"NOT SUPPORTED\"\n",
        "    }\n",
        "\n",
        "def test_query(query, expected_answer, api_url):\n",
        "    \"\"\"\n",
        "    Send a query to the API and print the answer, expected answer, and evaluation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{api_url}/api/query\",\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            json={\"query\": query}\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        if \"answer\" in result:\n",
        "            retrieved_chunks = retrieve_similar_chunks(query, model, index)\n",
        "            eval_result = evaluate_rag(query, expected_answer, retrieved_chunks)\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Actual Answer: {result['answer']}\")\n",
        "            print(f\"Expected Answer: {expected_answer}\")\n",
        "            print(f\"Evaluation: Groundedness={eval_result['grounded']}, Relevance (Avg Cosine Similarity)={eval_result['avg_similarity']}\")\n",
        "            print(\"-\" * 50)\n",
        "        else:\n",
        "            print(f\"Error for query '{query}': {result.get('error', 'Unknown error')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing query '{query}': {str(e)}\")\n",
        "\n",
        "def initialize_rag_system():\n",
        "    global model, index, chunks, openai_api_key\n",
        "    file_path = \"preprocessed_text.docx\"\n",
        "    pinecone_api_key = userdata.get(\"Pinecone_API_KEY\")\n",
        "    openai_api_key = userdata.get(\"my_GPT_key_2\")\n",
        "    ngrok_auth_token = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
        "\n",
        "    if not pinecone_api_key or not openai_api_key or not ngrok_auth_token:\n",
        "        raise ValueError(\"API keys must be set in Colab Secrets\")\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"'{file_path}' not found. Upload the file.\")\n",
        "\n",
        "    normalized_text = load_and_preprocess_docx(file_path)\n",
        "    chunks = chunk_text(normalized_text, language=\"bn\")\n",
        "    save_chunks(chunks)\n",
        "    model, index, chunks = initialize_vector_store(chunks, api_key=pinecone_api_key)\n",
        "\n",
        "def run_flask(port):\n",
        "    app.run(host='0.0.0.0', port=port)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "        terminate_port_processes(5000)\n",
        "        port = find_free_port()\n",
        "        initialize_rag_system()\n",
        "        ngrok.set_auth_token(userdata.get(\"NGROK_AUTH_TOKEN\"))\n",
        "        public_url = ngrok.connect(port)\n",
        "        api_url = public_url.public_url\n",
        "        print(f\"Public URL for API: {api_url}\")\n",
        "\n",
        "        # Start Flask server in a thread\n",
        "        flask_thread = threading.Thread(target=run_flask, args=(port,), daemon=True)\n",
        "        flask_thread.start()\n",
        "        time.sleep(2)  # Wait for Flask server to start\n",
        "\n",
        "        # Verify server is running\n",
        "        try:\n",
        "            response = requests.get(f\"http://localhost:{port}/api/query\", timeout=5)\n",
        "            if response.status_code != 405:  # Expect 405 (Method Not Allowed) for GET\n",
        "                raise Exception(\"Flask server not responding as expected\")\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            raise Exception(f\"Flask server failed to start on port {port}\")\n",
        "\n",
        "        # Corrected sample queries\n",
        "        sample_evals = [\n",
        "            {\"query\": \"অনুপমের ভাষায় সুপরুষ কাকে বলা হয়েছে?\", \"expected\": \"শম্ভুনাথ\"},\n",
        "            {\"query\": \"কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\", \"expected\": \"মামাকে\"},\n",
        "            {\"query\": \"Who is the main character in the story?\", \"expected\": \"অনুপম\"}\n",
        "        ]\n",
        "        for sample in sample_evals:\n",
        "            test_query(sample[\"query\"], sample[\"expected\"], api_url=api_url)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "jOs6TllU360c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}